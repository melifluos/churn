#!/usr/bin/env python -W ignore:DeprecationWarning

from __future__ import division
import pandas as pd
import numpy as np
import glob  # read files in dir
from sklearn.preprocessing import StandardScaler  # set all variables to sigma = 1, mu = 0
from sklearn.cross_validation import KFold, StratifiedKFold
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.feature_selection import SelectFromModel, chi2, SelectKBest, f_classif, RFECV
import time
from dateutil.parser import parse
from datetime import datetime
import warnings
from matplotlib import pyplot as plt
from visualisation import plot_confusion_matrix, plot_learning_curve, plot_roc, plot_pca, plot3d_pca
from sklearn.metrics import confusion_matrix


warnings.filterwarnings('ignore', category=DeprecationWarning)

__author__ = 'benchamberlain'


def ml():
    start = time.time()
    customers = pd.read_csv('local_resources/customer/000000_0', sep='\t')
    cust_columns = ['id', 'churn', 'gender', 'country', 'created_on', 'yob', 'premier']
    customers.columns = cust_columns
    customers.set_index('id', inplace=True)
    customers['churn'] -= 1
    customers.boxplot(['yob'])
    plt.savefig('local_results/yob.png', bbox_inches='tight')

    print 'read customer table in ', time.time() - start, 's'
    #  sample some data
    np.random.seed = 13253
    rows = np.random.choice(customers.index.values, 20000)
    customers = customers.ix[rows]

    #  add total number of purchases and total purchase value features
    receipts_data = process_receipts(customers.index.values)
    customers = customers.join(receipts_data, how='left')

    print 'added receipts in ', time.time() - start, 's'

    # add number of returns
    return_data = process_returns(customers.index.values)
    fig, axes = plt.subplots(nrows=1, ncols=1)
    return_data.boxplot(ax=axes)
    axes.set_title('returns')
    plt.savefig('local_results/returns.png', bbox_inches='tight')
    customers = customers.join(return_data, how='left')

    print 'added returns in ', time.time() - start, 's'

    # add web summaries
    web_data = process_weblogs()
    customers = customers.join(web_data, how='left')

    # fill nans generated by customers with no transactions of returns
    customers = customers.fillna(value=0)

    # remove accounts created in 1900
    customers = customers[customers['created_on'] != '1900-01-01T00:00:01.000Z']

    # FILTER FOR JUST GB
    customers = customers[customers['country'] == 'UK']
    # DO SOMETHING WITH DATE
    customers['account_duration'] = customers['created_on'].apply(parse_created_on)
    # map gender to 0,1
    customers['female'] = customers['gender'] == 'F'

    # add derived columns
    # customers['purchase freq'] = customers['']

    # Isolate target data
    y = np.array(customers['churn'])
    # We don't need these columns
    to_drop = ['churn', 'created_on', 'country', 'gender']
    churn_feat_space = customers.drop(to_drop, axis=1)
    features = churn_feat_space.columns.values
    X = churn_feat_space.as_matrix().astype(np.float)
    # print y.shape
    # print y

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # print X

    # perform feature selection
    # clf = RF().fit(X, y)
    #
    # print 'FEATURE IMPORTANCE'
    # feature_score = clf.feature_importances_
    # good_features = feature_score > 0.05
    # print 'removing {0} features'.format(len(feature_score) - np.count_nonzero(good_features))
    # print 'removing features {0}'.format(features[good_features])
    # model = SelectFromModel(clf, prefit=True)
    # X = model.transform(X)
    # X = X[:, good_features]
    # X = SelectKBest(f_classif, k=10).fit_transform(X, y)

###############################################################################################
    ## FEATURE SELECTION
#############################################################################################
    print "Feature space holds %d observations and %d features" % X.shape
    print "Unique target labels:", np.unique(y)
    rfecv = run_rfecv(X, y, RF)
    print 'features', features
    #print 'ranking', rfecv.ranking_
    #print 'support', rfecv.support_
    #print 'feature rank: ', rfecv.grid_scores_
    X = X[:, rfecv.support_]
#############################################################################################
    ### VISUALISATION
###############################################################################################

    plot3d_pca(X, y)
    X = plot_pca(X, y, class_names=['stayed', 'churned'])

    svc_pred = run_cv(X, y, SVC)
    rf_pred = run_cv(X, y, RF)
    knn_pred = run_cv(X, y, KNN)

    algos = [('SVM', SVC), ('RF', RF), ('KNN', KNN)]
    plot_roc(X, y, algos)

    print "Support vector machines:"
    print "%.3f" % accuracy(y, svc_pred)
    print "Random forest:"
    print "%.3f" % accuracy(y, rf_pred)
    print "K-nearest-neighbors:"
    print "%.3f" % accuracy(y, knn_pred)

    y = np.array(y)
    class_names = np.unique(y)

    plot_learning_curve(X, y, SVC(), 'Support vector machine')
    plot_learning_curve(X, y, RF(), 'Random forest')
    plot_learning_curve(X, y, KNN(), 'KNN')

    confusion_matrices = [
        ("SVM", confusion_matrix(y, svc_pred)),
        ("RF", confusion_matrix(y, rf_pred)),
        ("KNN", confusion_matrix(y, knn_pred)),
    ]

    plot_confusion_matrix(confusion_matrices, class_names)


CHURN_DIC = {1: 'ACTIVE', 2: 'CHURNED'}
PREMIER_DIC = {1: 'PENDING', 2: 'ACTIVE', 3: 'ACTIVE', 4: 'CANCELLED', 5: 'LAPSED', 6: 'DORMANT'}
DIVISION_DIC = {4: 'MENS_OUTLET', 5: 'MENS', 6: 'WOMENS_OUTLET', 7: 'WOMENS'}
SOURCE_DIC = {1: 'FULL_PRICE', 2: 'DISCOUNT_CODE', 3: 'SALES_PURCHASE', 4: 'OTHER', 10: 'RETURNS'}


def process_returns(ids):
    """
    process the returns table
    :return:
    """
    returns = pd.read_csv('local_resources/returns/000000_0', sep='\t')
    ret_columns = ['id', 'product_id', 'division', 'source', 'qty', 'date', 'receipt', 'return_id', 'return_action',
                   'return_reason']
    returns.columns = ret_columns

    # ret_drop_col = ['source', 'qty']  # constant across data
    # returns = returns.drop(ret_drop_col, axis=1)
    grouped_returns = returns[['id', 'return_id', 'return_action']].groupby(['id', 'return_action']).size()
    # try:
    #     grouped_returns = grouped_returns.drop(['return_action', 'id'], axis=1)
    # except KeyError:
    #     print 'no return action columns to remove'

    grouped_returns = grouped_returns.reset_index()
    grouped_returns.columns = ['id', 'return_action', 'count']
    return_counts = grouped_returns.pivot('id', 'return_action', 'count')
    return return_counts


def process_receipts(ids):
    """
    process receipts table
    :return:
    """
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    rec_columns = ['id', 'product_id', 'division', 'source', 'qty', 'date', 'receipt', 'price']
    receipts = read_dir('local_resources/receipts/0*', rec_columns)
    receipts = receipts[receipts['id'].isin(ids)]
    receipts['delta date'] = receipts['date'].apply(parse_created_on)
    grouped = receipts[['id', 'qty', 'price', 'delta date']].groupby('id').agg(
        {'qty': np.sum, 'price': np.sum, 'delta date': np.min})
    grouped.columns = ['days_since_last_receipt', 'total spend', 'total_items']
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    grouped.boxplot(ax=axes)
    plt.savefig('local_results/receipts_all.png', bbox_inches='tight')

    grouped_div = receipts[['id', 'division', 'qty', 'price']].groupby(['id', 'division']).sum()
    grouped_div = grouped_div.reset_index()
    div_qty = grouped_div.pivot('id', 'division', 'qty')
    div_qty.columns = ['div4_qty', 'div5_qty', 'div6_qty', 'div7_qty']
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    div_qty.boxplot(ax=axes)
    plt.savefig('local_results/receipts_div_qty.png', bbox_inches='tight')

    div_price = grouped_div.pivot('id', 'division', 'price')
    div_price.columns = ['div4_price', 'div5_price', 'div6_price', 'div7_price']
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    div_price.boxplot(ax=axes)
    plt.savefig('local_results/receipts_div_price.png', bbox_inches='tight')

    grouped_source = receipts[['id', 'source', 'qty', 'price']].groupby(['id', 'source']).sum()
    grouped_source = grouped_source.reset_index()
    source_qty = grouped_source.pivot('id', 'source', 'qty')
    source_qty.columns = ['source1_qty', 'source2_qty', 'source3_qty', 'source4_qty']
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    source_qty.boxplot(ax=axes)
    plt.savefig('local_results/receipts_source_qty.png', bbox_inches='tight')

    source_price = grouped_source.pivot('id', 'source', 'price')
    source_price.columns = ['source1_price', 'source2_price', 'source3_price', 'source4_price']
    fig, axes = plt.subplots(nrows=1, ncols=1)
    axes.set_title('receipts')
    source_price.boxplot(ax=axes)
    plt.savefig('local_results/receipts_source_price.png', bbox_inches='tight')

    # grouped_source = receipts[['id', 'division', 'qty', 'price']].groupby(['id', 'source']).sum()
    return pd.concat([div_qty, div_price, source_qty, source_price, grouped], axis=1)


def process_weblogs():
    """
    process the web summary data
    :return:
    """
    web_columns = ['id', 'country', 'start_time', 'site', 'page_view_count', 'event_count', 'user_agent', 'screen_res',
                   'browser_size', 'product_view_count', 'distinct_product_view_count', 'added_to_bag_count',
                   'product_saved_from_product_count',
                   'product_saved_from_category_count', 'distinct_products_purchased', 'total_products_purchased']

    web = read_dir('local_resources/sessionsummary/0*', web_columns)
    drop_columns = ['user_agent', 'screen_res', 'browser_size', 'start_time', 'site', 'country']  # not relevant

    web = web.drop(drop_columns, axis=1)

    grouped = web.groupby('id').sum()  #
    return grouped


def parse_created_on(row):
    """
    convert date to a time delta
    :param row: a row of the customer table
    :return: the time delta in days
    """
    row_date = parse(row)
    row_date = row_date.replace(tzinfo=None)
    diff = datetime.now() - row_date
    return diff.days


def read_dir(match_str, col_names):
    """
    Read all data in a folder into a single pandas DataFrame
    :param match_str:
    :return:
    """
    start = time.time()
    print 'matching for: ', match_str
    files = glob.glob(match_str)
    print 'files to run: ', files
    all_data = pd.DataFrame(columns=col_names)
    for count, f in enumerate(files):
        data = pd.read_csv(f, sep='\t', low_memory=False)
        data.columns = col_names
        all_data = pd.concat([all_data, data], axis=0)
        print count, ' files read in ', time.time() - start, ' seconds'
        print 'shape of data frame: ', all_data.shape
    return all_data


def read_data():
    """
    Read into pandas DataFrames
    :return:
    """
    customers = pd.read_csv('local_resources/customer/000000_0', sep='\t')
    cust_columns = ['id', 'churn', 'gender', 'country', 'created_on', 'yob', 'premier']
    customers.columns = cust_columns
    customers['churn'] -= 1

    web_columns = ['id', 'country', 'star_time', 'site', 'page_view_count', 'event_count', 'user_agent', 'screen_res',
                   'browser_size', 'product_view_count', 'distinct_product_view_count', 'added_to_bag_count',
                   'product_saved_from_product_count',
                   'product_saved_from_category_count', 'distinct_products_purchased', 'total_products_purchased']

    web = read_dir('local_resources/sessionsummary/0*', web_columns)
    drop_columns = ['user_agent', 'screen_res', 'browser_size']  # not relevant

    web.drop(drop_columns, axis=1, inplace=True)

    print 'web', web.shape
    print web.head()
    print 'receipts', receipts.shape
    print receipts.head()

    return web, receipts


def run_cv(X, y, clf_class, **kwargs):
    # Construct a kfolds object
    kf = StratifiedKFold(y, n_folds=5)
    y_pred = y.copy()

    # Iterate through folds
    for train_index, test_index in kf:
        X_train, X_test = X[train_index], X[test_index]
        y_train = y[train_index]
        # Initialize a classifier with key word arguments
        clf = clf_class(**kwargs)
        clf.fit(X_train, y_train)
        y_pred[test_index] = clf.predict(X_test)
    return y_pred


def plot_rfcev(rfecv):
    # Plot number of features VS. cross-validation scores
    plt.figure()
    plt.xlabel("Number of features selected")
    plt.ylabel("Cross validation score (nb of correct classifications)")
    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
    plt.savefig('local_results/feature_selection.png', bbox_inches='tight')


def run_rfecv(X, y, clf_class, **kwargs):
    clf = clf_class(**kwargs)
    rfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(y, 2), scoring='accuracy')
    rfecv.fit(X, y)
    plot_rfcev(rfecv)
    print "Optimal number of features : {0} for model: {1}".format(rfecv.n_features_, clf_class)
    return rfecv


def accuracy(y_true, y_pred):
    # NumPy interprets True and False as 1. and 0.
    return np.mean(y_true == y_pred)


if __name__ == '__main__':
    start = time.time()
    ml()
    print 'ran in time', time.time() - start, 's'
